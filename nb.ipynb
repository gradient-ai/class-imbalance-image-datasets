{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Setup and installations"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  article dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as Datasets\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from tqdm import tqdm as tqdm_regular\n",
    "import seaborn as sns\n",
    "from torchvision.utils import make_grid\n",
    "import random"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  setting up device\n",
    "if torch.cuda.is_available():\n",
    "  device = torch.device('cuda:0')\n",
    "  print('Running on the GPU')\n",
    "else:\n",
    "  device = torch.device('cpu')\n",
    "  print('Running on the CPU')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Evaluating the evalutation metrics\n",
    "\n",
    "## Loading in the training data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  loading training data\n",
    "training_set = Datasets.CIFAR10(root='./', download=True,\n",
    "                              transform=transforms.ToTensor())\n",
    "\n",
    "#  loading validation data\n",
    "validation_set = Datasets.CIFAR10(root='./', download=True, train=False,\n",
    "                                transform=transforms.ToTensor())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def extract_images(dataset):\n",
    "  cats = []\n",
    "  dogs = []\n",
    "\n",
    "  for idx in tqdm_regular(range(len(dataset))):\n",
    "    if dataset.targets[idx]==3:\n",
    "      cats.append((dataset.data[idx], 0))\n",
    "    elif dataset.targets[idx]==5:\n",
    "      dogs.append((dataset.data[idx], 1))\n",
    "    else:\n",
    "      pass\n",
    "  return cats, dogs\n",
    "  \n",
    "#  extracting training images\n",
    "cats, dogs = extract_images(training_set)\n",
    "#  creating training data with a 80:20 imbalance in favour of cats\n",
    "training_images = cats[:4800] + dogs[:1200]\n",
    "random.shuffle(training_images)\n",
    "\n",
    "#  extracting validation images\n",
    "cats, dogs = extract_images(validation_set)\n",
    "#  creating validation data\n",
    "validation_images = cats + dogs\n",
    "random.shuffle(validation_images)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Building a detector"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  defining dataset class\n",
    "class CustomCatsvsDogs(Dataset):\n",
    "  def __init__(self, data, transforms=None):\n",
    "    self.data = data\n",
    "    self.transforms = transforms\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.data)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    image = self.data[idx][0]\n",
    "    label = torch.tensor(self.data[idx][1])\n",
    "\n",
    "    if self.transforms!=None:\n",
    "      image = self.transforms(image)\n",
    "    return(image, label)\n",
    "    \n",
    "#  creating pytorch datasets\n",
    "training_data = CustomCatsvsDogs(training_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                 transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))\n",
    "validation_data = CustomCatsvsDogs(validation_images, transforms=transforms.Compose([transforms.ToTensor(),\n",
    "                                                                                     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Training a Mock Model"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  creating a mock model\n",
    "def mock_model(image_instance, batch_mode=False):\n",
    "  \"\"\"\n",
    "  This function serves as a mock model which classifies\n",
    "  all images as 0. If batch_mode=True supply a list of image-label\n",
    "  pairs as parameter image_instance.\n",
    "  \"\"\"\n",
    "  if not batch_mode:\n",
    "    classification = 0\n",
    "    label = image_instance[1].item()\n",
    "    print(f'model classification = {classification}\\ntrue label = {label}')\n",
    "\n",
    "  else:\n",
    "    #  extracting true labels\n",
    "    labels = [x[1] for x in image_instance]\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    #  classifying all instances as class 0\n",
    "    classifications = [0]*(len(image_instance))\n",
    "    classifications = np.array(classifications)\n",
    "\n",
    "    #  computing accuracy\n",
    "    accuracy = (sum(labels==classifications)/len(labels))\n",
    "    print('model accuracy:')\n",
    "    return round(accuracy, 3)\n",
    "    \n",
    "\n",
    "#  testing model in batch mode\n",
    "mock_model(training_data, batch_mode=True)\n",
    ">>>> model accuracy:\n",
    "     0.8\n",
    "     \n",
    "#  testing model in batch mode\n",
    "mock_model(validation_data, batch_mode=True)\n",
    ">>>> model accuracy:\n",
    "     0.5"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate our Convnet class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvNet(nn.Module):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.conv1 = nn.Conv2d(3, 8, 3, padding=1)\n",
    "    self.batchnorm1 = nn.BatchNorm2d(8)\n",
    "    self.conv2 = nn.Conv2d(8, 8, 3, padding=1)\n",
    "    self.batchnorm2 = nn.BatchNorm2d(8)\n",
    "    self.pool2 = nn.MaxPool2d(2)\n",
    "    self.conv3 = nn.Conv2d(8, 32, 3, padding=1)\n",
    "    self.batchnorm3 = nn.BatchNorm2d(32)\n",
    "    self.conv4 = nn.Conv2d(32, 32, 3, padding=1)\n",
    "    self.batchnorm4 = nn.BatchNorm2d(32)\n",
    "    self.pool4 = nn.MaxPool2d(2)\n",
    "    self.conv5 = nn.Conv2d(32, 128, 3, padding=1)\n",
    "    self.batchnorm5 = nn.BatchNorm2d(128)\n",
    "    self.conv6 = nn.Conv2d(128, 128, 3, padding=1)\n",
    "    self.batchnorm6 = nn.BatchNorm2d(128)\n",
    "    self.pool6 = nn.MaxPool2d(2)\n",
    "    self.conv7 = nn.Conv2d(128, 2, 1)\n",
    "    self.pool7 = nn.AvgPool2d(3)\n",
    "\n",
    "  def forward(self, x):\n",
    "    #-------------\n",
    "    # INPUT\n",
    "    #-------------\n",
    "    x = x.view(-1, 3, 32, 32)\n",
    "    \n",
    "    #-------------\n",
    "    # LAYER 1\n",
    "    #-------------\n",
    "    output_1 = self.conv1(x)\n",
    "    output_1 = F.relu(output_1)\n",
    "    output_1 = self.batchnorm1(output_1)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 2\n",
    "    #-------------\n",
    "    output_2 = self.conv2(output_1)\n",
    "    output_2 = F.relu(output_2)\n",
    "    output_2 = self.pool2(output_2)\n",
    "    output_2 = self.batchnorm2(output_2)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 3\n",
    "    #-------------\n",
    "    output_3 = self.conv3(output_2)\n",
    "    output_3 = F.relu(output_3)\n",
    "    output_3 = self.batchnorm3(output_3)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 4\n",
    "    #-------------\n",
    "    output_4 = self.conv4(output_3)\n",
    "    output_4 = F.relu(output_4)\n",
    "    output_4 = self.pool4(output_4)\n",
    "    output_4 = self.batchnorm4(output_4)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 5\n",
    "    #-------------\n",
    "    output_5 = self.conv5(output_4)\n",
    "    output_5 = F.relu(output_5)\n",
    "    output_5 = self.batchnorm5(output_5)\n",
    "\n",
    "    #-------------\n",
    "    # LAYER 6\n",
    "    #-------------\n",
    "    output_6 = self.conv6(output_5)\n",
    "    output_6 = F.relu(output_6)\n",
    "    output_6 = self.pool6(output_6)\n",
    "    output_6 = self.batchnorm6(output_6)\n",
    "\n",
    "    #--------------\n",
    "    # OUTPUT LAYER\n",
    "    #--------------\n",
    "    output_7 = self.conv7(output_6)\n",
    "    output_7 = self.pool7(output_7)\n",
    "    output_7 = output_7.view(-1, 2)\n",
    "\n",
    "    return F.softmax(output_7, dim=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Instantiate our ConvolutionalNeuralNet class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalNeuralNet():\n",
    "  def __init__(self, network):\n",
    "    self.network = network.to(device)\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  defining accuracy function\n",
    "    def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      return round(total_correct/total_instances, 3)\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      for images, labels in tqdm(train_loader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        self.optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy = accuracy(self.network, train_loader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      self.network.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = self.network(images)\n",
    "          #  computing loss\n",
    "          val_loss = loss_function(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy = accuracy(self.network, val_loader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}\\n')\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def predict(self, x):\n",
    "    return self.network(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model training"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalNeuralNet(ConvNet())\n",
    "\n",
    "log_dict = model.train(nn.CrossEntropyLoss(), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Handling class imbalance with class weights\n",
    "\n",
    "### Instantiate our ConvolutionalNeuralNet_2 class"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "class ConvolutionalNeuralNet_2():\n",
    "  def __init__(self, network):\n",
    "    self.network = network.to(device)\n",
    "    self.optimizer = torch.optim.Adam(self.network.parameters(), lr=1e-3)\n",
    "\n",
    "  def train(self, loss_function, epochs, batch_size, \n",
    "            training_set, validation_set):\n",
    "    \n",
    "    #  creating log\n",
    "    log_dict = {\n",
    "        'training_loss_per_batch': [],\n",
    "        'validation_loss_per_batch': [],\n",
    "        'training_accuracy_per_epoch': [],\n",
    "        'training_recall_per_epoch': [],\n",
    "        'training_precision_per_epoch': [],\n",
    "        'validation_accuracy_per_epoch': [],\n",
    "        'validation_recall_per_epoch': [],\n",
    "        'validation_precision_per_epoch': []\n",
    "    } \n",
    "\n",
    "    #  defining weight initialization function\n",
    "    def init_weights(module):\n",
    "      if isinstance(module, nn.Conv2d):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "      elif isinstance(module, nn.Linear):\n",
    "        torch.nn.init.xavier_uniform_(module.weight)\n",
    "        module.bias.data.fill_(0.01)\n",
    "\n",
    "    #  defining accuracy function\n",
    "    def accuracy(network, dataloader):\n",
    "      network.eval()\n",
    "      \n",
    "      all_predictions = []\n",
    "      all_labels = []\n",
    "\n",
    "      #  computing accuracy\n",
    "      total_correct = 0\n",
    "      total_instances = 0\n",
    "      for images, labels in tqdm(dataloader):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        all_labels.extend(labels)\n",
    "        predictions = torch.argmax(network(images), dim=1)\n",
    "        all_predictions.extend(predictions)\n",
    "        correct_predictions = sum(predictions==labels).item()\n",
    "        total_correct+=correct_predictions\n",
    "        total_instances+=len(images)\n",
    "      accuracy = round(total_correct/total_instances, 3)\n",
    "\n",
    "      #  computing recall and precision\n",
    "      true_positives = 0\n",
    "      false_negatives = 0\n",
    "      false_positives = 0\n",
    "      for idx in range(len(all_predictions)):\n",
    "        if all_predictions[idx].item()==1 and  all_labels[idx].item()==1:\n",
    "          true_positives+=1\n",
    "        elif all_predictions[idx].item()==0 and all_labels[idx].item()==1:\n",
    "          false_negatives+=1\n",
    "        elif all_predictions[idx].item()==1 and all_labels[idx].item()==0:\n",
    "          false_positives+=1\n",
    "      try:\n",
    "        recall = round(true_positives/(true_positives + false_negatives), 3)\n",
    "      except ZeroDivisionError:\n",
    "        recall = 0.0\n",
    "      try:\n",
    "        precision = round(true_positives/(true_positives + false_positives), 3)\n",
    "      except ZeroDivisionError:\n",
    "        precision = 0.0\n",
    "      return accuracy, recall, precision\n",
    "\n",
    "    #  initializing network weights\n",
    "    self.network.apply(init_weights)\n",
    "\n",
    "    #  creating dataloaders\n",
    "    train_loader = DataLoader(training_set, batch_size)\n",
    "    val_loader = DataLoader(validation_set, batch_size)\n",
    "\n",
    "    #  setting convnet to training mode\n",
    "    self.network.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "      print(f'Epoch {epoch+1}/{epochs}')\n",
    "      train_losses = []\n",
    "\n",
    "      #  training\n",
    "      print('training...')\n",
    "      for images, labels in tqdm(train_loader):\n",
    "        #  sending data to device\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        #  resetting gradients\n",
    "        self.optimizer.zero_grad()\n",
    "        #  making predictions\n",
    "        predictions = self.network(images)\n",
    "        #  computing loss\n",
    "        loss = loss_function(predictions, labels)\n",
    "        log_dict['training_loss_per_batch'].append(loss.item())\n",
    "        train_losses.append(loss.item())\n",
    "        #  computing gradients\n",
    "        loss.backward()\n",
    "        #  updating weights\n",
    "        self.optimizer.step()\n",
    "      with torch.no_grad():\n",
    "        print('deriving training accuracy...')\n",
    "        #  computing training accuracy\n",
    "        train_accuracy, train_recall, train_precision = accuracy(self.network, train_loader)\n",
    "        log_dict['training_accuracy_per_epoch'].append(train_accuracy)\n",
    "        log_dict['training_recall_per_epoch'].append(train_recall)\n",
    "        log_dict['training_precision_per_epoch'].append(train_precision)\n",
    "\n",
    "      #  validation\n",
    "      print('validating...')\n",
    "      val_losses = []\n",
    "\n",
    "      #  setting convnet to evaluation mode\n",
    "      self.network.eval()\n",
    "\n",
    "      with torch.no_grad():\n",
    "        for images, labels in tqdm(val_loader):\n",
    "          #  sending data to device\n",
    "          images, labels = images.to(device), labels.to(device)\n",
    "          #  making predictions\n",
    "          predictions = self.network(images)\n",
    "          #  computing loss\n",
    "          val_loss = loss_function(predictions, labels)\n",
    "          log_dict['validation_loss_per_batch'].append(val_loss.item())\n",
    "          val_losses.append(val_loss.item())\n",
    "        #  computing accuracy\n",
    "        print('deriving validation accuracy...')\n",
    "        val_accuracy, val_recall, val_precision = accuracy(self.network, val_loader)\n",
    "        log_dict['validation_accuracy_per_epoch'].append(val_accuracy)\n",
    "        log_dict['validation_recall_per_epoch'].append(val_recall)\n",
    "        log_dict['validation_precision_per_epoch'].append(val_precision)\n",
    "\n",
    "      train_losses = np.array(train_losses).mean()\n",
    "      val_losses = np.array(val_losses).mean()\n",
    "\n",
    "      print(f'training_loss: {round(train_losses, 4)}  training_accuracy: '+\n",
    "      f'{train_accuracy}  training_recall: {train_recall}  training_precision: {train_precision} *~* validation_loss: {round(val_losses, 4)} '+  \n",
    "      f'validation_accuracy: {val_accuracy}  validation_recall: {val_recall}  validation_precision: {val_precision}\\n')\n",
    "      \n",
    "    return log_dict\n",
    "\n",
    "  def predict(self, x):\n",
    "    return self.network(x)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## An unbalanced approach:\n",
    "\n",
    " weight of 0 has been assigned to class 0 and a weight of 1.0 has been assigned to class 1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalNeuralNet_2(ConvNet())\n",
    "\n",
    "weight = torch.tensor((0, 1.0))\n",
    "\n",
    "log_dict = model.train(nn.CrossEntropyLoss(weight=weight), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## A more balanced approach"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#  training model\n",
    "model = ConvolutionalNeuralNet_2(ConvNet())\n",
    "\n",
    "weight = torch.tensor((0.15, 0.85))\n",
    "\n",
    "log_dict = model.train(nn.CrossEntropyLoss(weight=weight), epochs=10, batch_size=64, \n",
    "                       training_set=training_data, validation_set=validation_data)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}